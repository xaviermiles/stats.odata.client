% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fct_odata_get.R
\name{parallel_get}
\alias{parallel_get}
\title{Parallelised version of basic_get, for requesting larger amounts of data.}
\usage{
parallel_get(
  endpoint,
  entity = "",
  query = "",
  timeout = 10,
  splitting_col = "ResourceID",
  max_cores = 4,
  rows_per_query = 10000
)
}
\arguments{
\item{endpoint}{API endpoint. Required.}

\item{entity}{Data entity.}

\item{query}{Query URL character (not URL-encoded).}

\item{timeout}{Timeout for the GET request(s), in seconds.}

\item{splitting_col}{The column on which to split the overall request into
bite-size portions.}

\item{max_cores}{Maximum number of cores. This will be overruled if there is
less cores available.}

\item{rows_per_query}{The approx. number of rows per individual request.
This can be used to tune performance.}
}
\value{
A data frame containing the requested data.
}
\description{
This uses multiples cores to make concurrent API requests, and
then merges the individual results. There is some upfront work required to
determine the series of smaller requests, so this function shouldn't be used
for "small" requests.

FIXME: support `query` that include $filter, this would require merging it
       with the $filter generated with the `splitting_col`.
}
